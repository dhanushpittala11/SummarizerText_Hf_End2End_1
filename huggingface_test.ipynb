{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed. CSV files are stored in: /home/dhanush/billsum\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input directory containing .parquet files\n",
    "input_dir = \"/home/dhanush/billsum/data\"\n",
    "output_dir = \"/home/dhanush/billsum\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert .parquet files to .csv\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".parquet\"):\n",
    "        df = pd.read_parquet(os.path.join(input_dir, file))\n",
    "        csv_file = file.replace(\".parquet\", \".csv\")\n",
    "        df.to_csv(os.path.join(output_dir, csv_file), index=False,escapechar='\\\\')\n",
    "\n",
    "print(\"Conversion completed. CSV files are stored in:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 18949 examples [00:01, 10515.06 examples/s]\n",
      "Generating train split: 3269 examples [00:00, 11141.97 examples/s]\n",
      "Generating train split: 1237 examples [00:00, 9469.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 18949/18949 [00:00<00:00, 95358.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3269/3269 [00:00<00:00, 57474.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1237/1237 [00:00<00:00, 58907.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to /home/dhanush/billsum/huggingface_billsum_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "csv_dir = \"/home/dhanush/billsum\"\n",
    "hf_dataset_dir = \"/home/dhanush/billsum/huggingface_billsum_dataset\"\n",
    "\n",
    "# Create the dataset splits\n",
    "train_csv = os.path.join(csv_dir, \"train-00000-of-00001.csv\")\n",
    "test_csv = os.path.join(csv_dir, \"test-00000-of-00001.csv\")\n",
    "val_csv = os.path.join(csv_dir, \"ca_test-00000-of-00001.csv\")\n",
    "\n",
    "# Load the CSV files as Hugging Face datasets\n",
    "train_dataset = Dataset.from_csv(train_csv)\n",
    "test_dataset = Dataset.from_csv(test_csv)\n",
    "val_dataset = Dataset.from_csv(val_csv)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "})\n",
    "\n",
    "# Save the dataset to disk\n",
    "os.makedirs(hf_dataset_dir, exist_ok=True)\n",
    "dataset_dict.save_to_disk(hf_dataset_dir)\n",
    "\n",
    "print(f\"Dataset saved to {hf_dataset_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 12460 examples [00:00, 101791.20 examples/s]\n",
      "Generating train split: 1500 examples [00:00, 115173.29 examples/s]\n",
      "Generating train split: 500 examples [00:00, 67517.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12460/12460 [00:00<00:00, 749573.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1500/1500 [00:00<00:00, 305603.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 141680.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to /home/dhanush/dialogsum/huggingface_dialogsum_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "csv_dir = \"/home/dhanush/dialogsum\"\n",
    "hf_dataset_dir = \"/home/dhanush/dialogsum/huggingface_dialogsum_dataset\"\n",
    "\n",
    "# Create the dataset splits\n",
    "train_csv = os.path.join(csv_dir, \"train.csv\")\n",
    "test_csv = os.path.join(csv_dir, \"test.csv\")\n",
    "val_csv = os.path.join(csv_dir, \"validation.csv\")\n",
    "\n",
    "# Load the CSV files as Hugging Face datasets\n",
    "train_dataset = Dataset.from_csv(train_csv)\n",
    "test_dataset = Dataset.from_csv(test_csv)\n",
    "val_dataset = Dataset.from_csv(val_csv)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "})\n",
    "\n",
    "# Save the dataset to disk\n",
    "os.makedirs(hf_dataset_dir, exist_ok=True)\n",
    "dataset_dict.save_to_disk(hf_dataset_dir)\n",
    "\n",
    "print(f\"Dataset saved to {hf_dataset_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
